import os

import asyncio

from langchain_community.vectorstores import FAISS
from langchain_openai import AzureOpenAIEmbeddings
from pydantic import SecretStr

from utils.page_utils import get_faiss_index_dir


async def create_and_save_vector_store(texts: list[str], scrape_id: str):
    """
    Creates a FAISS vector store from text chunks and saves it locally
    for a specific scrape_id.
    """
    # Ensure OpenAI API key is available
    api_key = os.getenv("API_KEY")
    if not api_key:
        raise ValueError("API_KEY environment variable not set.")

    faiss_index_dir = get_faiss_index_dir(scrape_id)
    os.makedirs(faiss_index_dir, exist_ok=True)

    # Initialize OpenAI Embeddings model
    # This model converts text into numerical vector representations.
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=os.getenv("MODEL_NAME"),
        azure_endpoint=os.getenv("MODEL_URL"),
        api_version=os.getenv("MODEL_VERSION"),
        api_key=SecretStr(api_key)
    )

    # Create a FAISS vector store from the text chunks and their embeddings.
    # FAISS is an efficient library for similarity search.
    vectorstore = await asyncio.to_thread(FAISS.from_texts, texts, embeddings)

    # Save the FAISS index locally. This creates two files:
    # 1. db_path/index.faiss (the FAISS index itself)
    # 2. db_path/index.pkl (metadata, including the texts and embeddings)
    await asyncio.to_thread(vectorstore.save_local, faiss_index_dir)
    print(f"Vector store successfully created and saved for scrape_id {scrape_id} to {faiss_index_dir}")


async def load_vector_store(scrape_id: str):
    """
    Loads an existing FAISS vector store from a local directory for a specific scrape_id.
    """
    faiss_index_dir = get_faiss_index_dir(scrape_id)
    if not os.path.exists(faiss_index_dir):
        raise FileNotFoundError(f"FAISS index for scrape_id {scrape_id} not found at {faiss_index_dir}")

    # Ensure OpenAI API key is available
    api_key = os.getenv("API_KEY")
    if not api_key:
        raise ValueError("API_KEY environment variable not set.")

    # Initialize OpenAI Embeddings model (must be the same model used for creation)
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=os.getenv("MODEL_NAME"),
        azure_endpoint=os.getenv("MODEL_URL"),
        api_version=os.getenv("MODEL_VERSION"),
        api_key=SecretStr(api_key)
    )

    # Load the FAISS index from the specified path.
    # allow_dangerous_deserialization=True is necessary for loading pickle files
    # generated by older versions or from untrusted sources. Use with caution.
    vectorstore = await asyncio.to_thread(
        FAISS.load_local, faiss_index_dir, embeddings, allow_dangerous_deserialization=True
    )
    print(f"Vector store successfully loaded for scrape_id {scrape_id} from {faiss_index_dir}")
    return vectorstore
